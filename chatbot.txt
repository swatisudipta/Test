from llama_cpp import Llama
from typing import Generator
import os

class DevOpsChatbot:
    def __init__(self):
        self.model = self._load_model()
        self.system_prompt = """<<SYS>>
You are a helpful DevOps expert assistant. Answer questions about:
- CI/CD pipelines (Jenkins, GitHub Actions, Azure DevOps)
- Cloud platforms (AWS, Azure, GCP)
- Infrastructure as Code (Terraform, Ansible)
- Containerization (Docker, Kubernetes)
- Monitoring and logging
Provide concise, accurate answers with examples when possible.
<</SYS>>"""

    def _load_model(self):
        model_path = "C:/Users/2742425/Desktop/GenAi-Generation/LLM-Models/llama-2-7b-chat.Q4_K_M.gguf"
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model not found at {model_path}")
        
        return Llama(
            model_path=model_path,
            n_ctx=2048,
            n_threads=4,
            n_gpu_layers=40
        )

    def generate_response(self, user_input: str) -> Generator[str, None, None]:
        """Stream generated response"""
        prompt = f"[INST]{self.system_prompt}\n{user_input}[/INST]"
        
        response = self.model(
            prompt,
            max_tokens=512,
            temperature=0.7,
            top_p=0.9,
            stream=True
        )
        
        for chunk in response:
            yield chunk["choices"][0]["text"]

# Singleton instance
chatbot_instance = DevOpsChatbot()